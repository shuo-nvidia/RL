defaults: ../../dpo.yaml
dpo:
  val_global_batch_size: 32
  val_at_start: false
policy:
  tokenizer:
    name: ${policy.model_name}
logger:
  wandb_enabled: true
  tensorboard_enabled: true
  wandb:
    project: nemo-rl
    name: dpo-llama3.2-1b-instruct-1n8g-fsdp2tp1
cluster:
  gpus_per_node: 8
