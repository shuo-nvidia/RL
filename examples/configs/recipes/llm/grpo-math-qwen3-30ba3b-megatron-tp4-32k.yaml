defaults: ../../grpo_math_1B.yaml
checkpointing:
  checkpoint_dir: results/grpo-math-qwen3-30ba3b-megatron-tp4-32k
  save_period: 3
  keep_top_k: 1
grpo:
  max_num_steps: 3
  num_prompts_per_step: 64
  val_period: 3
policy:
  model_name: Qwen/Qwen3-30B-A3B
  train_micro_batch_size: 1
  logprob_batch_size: 1
  max_total_sequence_length: 32768
  logprob_chunk_size: 2048
  dtensor_cfg:
    enabled: false
  sequence_packing:
    enabled: false
  make_sequence_length_divisible_by: ${policy.megatron_cfg.tensor_model_parallel_size}
  optimizer: null
  scheduler: null
  megatron_cfg:
    enabled: true
    empty_unused_memory_level: 1
    converter_type: LlamaForCausalLM
    tensor_model_parallel_size: 4
    expert_model_parallel_size: 8
    sequence_parallel: true
    activation_checkpointing: true
    defer_fp32_logits: true
    optimizer:
      lr: 5.0e-07
      min_lr: 5.0e-08
      weight_decay: 0.0
    scheduler:
      lr_warmup_iters: 2
      lr_warmup_init: 5.0e-08
  generation:
    vllm_cfg:
      tensor_parallel_size: 4
      enforce_eager: true
logger:
  log_dir: logs/grpo-math-qwen3-30ba3b-megatron-tp4-32k
  wandb_enabled: true
  tensorboard_enabled: true
  monitor_gpus: false
  wandb:
    project: nemo-rl
    name: grpo-math-qwen3-30ba3b-megatron-tp4-32k
cluster:
  gpus_per_node: 8
  num_nodes: 4
