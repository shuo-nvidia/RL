defaults: distillation_math.yaml

distillation:
    num_prompts_per_step: 512
    num_generations_per_prompt: 1
    max_num_steps: 1000
    val_batch_size: 512
    val_period: 20
    val_at_start: false
    topk_logits_k: 64

loss_fn:
    kl_type: "reverse" # forward, reverse, mixed
    smooth_correction: true # whether to use smooth correction for alignment correction term
    sample_level_correlation: false # whether to enable sample level weighting based on teacher PPL
    token_level_correlation: false # whether to enable token level weighting based on student not in teacher count
    teacher_eos_token_id: 151645 # Qwen3-32B uses <|im_end|> as EOS
    student_eos_token_id: 151643 # Qwen3-4B-Base uses <|endoftext|> as EOS

checkpointing:
    checkpoint_dir: "checkpoints/distillation-ocv2-32B-${policy.model_name}"
    save_period: 5

policy:
    model_name: "Qwen/Qwen3-1.7B-Base"
    max_total_sequence_length: 20480
    tokenizer:
        name: "Qwen/Qwen3-1.7B-Base" ## specify if you'd like to use a tokenizer different from the model's default
    train_global_batch_size: 512

    dtensor_cfg:
        tensor_parallel_size: 4
        context_parallel_size: 1

    optimizer:
        kwargs:
            lr: 2.0e-5

teacher:
    model_name: "Qwen/Qwen3-4B-Thinking-2507"
    dtensor_cfg:
        tensor_parallel_size: 4
        context_parallel_size: 1

logger:
    wandb:
        project: "alex-distillation"
        name: "distillation-${data.dataset_name}-${teacher.model_name}-${policy.model_name}-${loss_fn.kl_type}-${distillation.topk_logits_k}-${loss_fn.zero_outside_topk}"
